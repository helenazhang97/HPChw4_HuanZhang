1.I ran 4 processes on 2 nodes, each MPI process on a CPU core.
When I communicate between two nodes, the result is as follows:
  1 proc0= 0, proc1= 3 Rank 0/4 running on cs059.nyu.cluster.
  2 proc0= 0, proc1= 3 Rank 1/4 running on cs059.nyu.cluster.
  3 proc0= 0, proc1= 3 Rank 2/4 running on cs060.nyu.cluster.
  4 proc0= 0, proc1= 3 Rank 3/4 running on cs060.nyu.cluster.
  5 pingpong latency: 5.730687e-03 ms
  6 pingpong bandwidth: 1.287487e+01 GB/s

When I communicate between two cores on a single node, the result is as follows:
  1 proc0= 0, proc1= 1 Rank 2/4 running on cs005.nyu.cluster.
  2 proc0= 0, proc1= 1 Rank 3/4 running on cs005.nyu.cluster.
  3 proc0= 0, proc1= 1 Rank 0/4 running on cs004.nyu.cluster.
  4 proc0= 0, proc1= 1 Rank 1/4 running on cs004.nyu.cluster.
  5 pingpong latency: 3.464010e-04 ms
  6 pingpong bandwidth: 2.471767e+01 GB/s

We could see that the communication between codes through network requires more time. It also shows that at least for the specific case for Greene, when sending (as often) small messages, then what matters is the latency, not the bandwith.



2.(a) The output of checking if after N loops the processors have properly added their distribution each time. Please note that my Rank 0/4 sums up first but print its share only after it receives the result from the last processor, so it print out result later(although it sent its message first). This version of code is not submitted, just built for checking.

Rank 1/4 running on log-3.nyu.cluster, receive 0, add 1, send 1.
Rank 2/4 running on log-3.nyu.cluster, receive 1, add 2, send 3.
Rank 3/4 running on log-3.nyu.cluster, receive 3, add 3, send 6.
Rank 0/4 running on log-3.nyu.cluster, add 0, send 0
Rank 3/4 running on log-3.nyu.cluster, receive 9, add 3, send 12.
Rank 1/4 running on log-3.nyu.cluster, receive 6, add 1, send 7.
Rank 2/4 running on log-3.nyu.cluster, receive 7, add 2, send 9.
Rank 0/4 running on log-3.nyu.cluster, add 0, send 6
after 2 loop/loops N= 12.

(b) I let N=100 and use 4 MPI processes. I first run them on 4 different nodes
Rank 1/4 running on cs113.nyu.cluster
Rank 3/4 running on cs201.nyu.cluster
Rank 0/4 running on cs112.nyu.cluster
Rank 2/4 running on cs200.nyu.cluster
After 100 loop/loops N= 600, latency:4.500549e-02 ms

Then I run them on 1 node with 4 different cores
Rank 3/4 running on cs004.nyu.cluster
Rank 2/4 running on cs004.nyu.cluster
Rank 1/4 running on cs004.nyu.cluster
Rank 0/4 running on cs004.nyu.cluster
After 100 loop/loops N= 600, latency:3.193725e-04 ms

We can see that when the messages are not actually set through a network, the latency is smaller.

(c) please refer to int_ring.c 
(d) 
If I run 4 MPI prcesses on one node (4 different cores)
Rank 3/4 running on cs016.nyu.cluster
Rank 0/4 running on cs016.nyu.cluster
Rank 1/4 running on cs016.nyu.cluster
Rank 2/4 running on cs016.nyu.cluster
After 100 loop/loops N= 3, bandwidth:1.101590e+01 GB/s

If I run 4 MPI processes on 4 different nodes 
Rank 0/4 running on cs227.nyu.cluster
Rank 2/4 running on cs229.nyu.cluster
Rank 3/4 running on cs230.nyu.cluster
Rank 1/4 running on cs228.nyu.cluster
After 100 loop/loops N= 3, bandwidth:5.321670e+00 GB/s

We can see the bandwidth is reduced by approximately a half when using networks.
3. I picked(b) Jacobi iteration in 2D(I only did MPI parallel implementation, no OpenMP yet)
When N=256(the points in each dimension) and tolorance =1e-6, using 2 x 2 MPI processes on 1 node on Greene,Time elapsed is 0.259291 seconds.
When N=256(the points in each dimension) and tolorance =1e-6, using 4 x 4 MPI processes on 1 node on Greene, Time elapsed is 0.030859 seconds.
When N=256(the points in each dimension) and tolorance =1e-6, using 8 x 8 MPI processes equally distributed on 2 nodes on Greene,Time elapsed is 0.014057 seconds.
When N=256(the points in each dimension) and tolorance =1e-6, using 8 x 8 MPI processes equally distributed on 4 nodes on Greene,time elapsed is 0.010732 seconds.

We can see when we increase the number of MPI processes by a factor of 4 within 1 node, the time is reduced by more than a factor of 4. But when we are using more than one node, the communication between nodes add to the time consumed because of latency. Moreover, as the 3rd and 4th implementation shows, even though there is possibly more time spent on communication between more nodes(4 outnumber 2), more computation resource still make the 4th implementation faster (there are 24 nodes in each node so hyperthreading is used in 3rd case.)

4. The project. We are planning to computes 1d, 2d and possibly 3d FFTs in parallel(Openmp) in each dimension of the FFT grid, interleaved with MPI communication to move data between processors. I will be cooperating with Cai Maitland-Davies. 
